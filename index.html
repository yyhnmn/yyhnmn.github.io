<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/ML_notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/ML_notes/" class="post-title-link" itemprop="url">Machine Learning notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-16 17:24:36" itemprop="dateCreated datePublished" datetime="2020-09-16T17:24:36-06:00">2020-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-26 20:14:49" itemprop="dateModified" datetime="2019-12-26T20:14:49-07:00">2019-12-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>

<h1 id="MLE-and-MAP"><a href="#MLE-and-MAP" class="headerlink" title="MLE and MAP"></a>MLE and MAP</h1><p>Maximum Likelihood Estimation(MLE) and Maximum a Posteriori(MAP) are both methods to find the best model for the observed data. The difference is the inclusion of prior $p(\theta)$ in MAP. We can say MLE and MAP are identical if $\theta$ is uniform distributed. The formula for MLE and MAP are given below<br>$$<br>\begin{aligned}<br>\theta_{MLE}&amp;=\underset{\theta}{\mathrm{argmax}} \ p(D|\theta)\\<br>&amp;=\underset{\theta}{\mathrm{argmax}} \ \prod_{i=1}^{n} p(x_i|\theta)<br>\end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned}<br>\theta_{MAP}&amp;=\underset{\theta}{\mathrm{argmax}} \ p(\theta|D)\\<br>&amp;=\underset{\theta}{\mathrm{argmax}} \ p(D|\theta)p(\theta)\\<br>&amp;=\underset{\theta}{\mathrm{argmax}} \ \prod_{i=1}^{n} p(x_i|\theta)p(\theta)<br>\end{aligned}<br>$$ </p>
<h1 id="Reducible-and-Irreducible-Error"><a href="#Reducible-and-Irreducible-Error" class="headerlink" title="Reducible and Irreducible Error"></a>Reducible and Irreducible Error</h1><p>For a squared error cost<br>$$<br>\mathop{\mathbb{E}[C]} = \int_{X} p(x) \int_{Y} (f(x)-y)^2p(y|x)dydx<br>$$<br>There are two situations:</p>
<ol>
<li>$f(x) = \mathop{\mathbb{E}[Y|x]}$</li>
<li>$f(x) \neq \mathop{\mathbb{E}[Y|x]}$</li>
</ol>
<p>When $f(x) = \mathop{\mathbb{E}[Y|x]}$, the expected cost can be simply expressed as<br>$$<br>\begin{aligned}<br>\mathop{\mathbb{E}[C]} &amp;= \int_{X} p(x) \int_{Y} (\mathop{\mathbb{E}[Y|x]}-y)^2p(y|x)dydx\\<br>&amp;= \int_{X} p(x)V[Y|X=x]dx<br>\end{aligned}<br>$$<br>The expected cost reflects the cost incurred from noise or variability in the targets so that this is the best scenario in regression for a squared error cost and we cannot achieve a lover expected cost.</p>
<p>When $f(x) \neq \mathop{\mathbb{E}[Y|x]}$, we need to proceed by decomposing the squared error as<br>$$<br>\begin{aligned}<br>(f(x)-y)^2 &amp;= (f(x)- \mathop{\mathbb{E}[Y|x]} +\mathop{\mathbb{E}[Y|x]} -y )^2\\<br>&amp;= (f(x)- \mathop{\mathbb{E}[Y|x]})^2 + 2(f(x)- \mathop{\mathbb{E}[Y|x]})(\mathop{\mathbb{E}[Y|x]}-y)+(\mathop{\mathbb{E}[Y|x]}-y)^2<br>\end{aligned}<br>$$<br>let $g(x,y) = (f(x)- \mathop{\mathbb{E}[Y|x]})(\mathop{\mathbb{E}[Y|x]}-y)$, we can notice that the expected value of g(x,Y) for each x is zero because<br>$$<br>\begin{aligned}<br>\mathop{\mathbb{E}[g(x,Y)]} &amp;= \mathop{\mathbb{E}[(f(x)- \mathop{\mathbb{E}[Y|x]})(\mathop{\mathbb{E}[Y|x]}-Y)|x]}\\<br>&amp;= (f(x)- \mathop{\mathbb{E}[Y|x]})\mathop{\mathbb{E}[\mathop{\mathbb{E}[Y|x]}-Y)|x]}\\<br>&amp;=(f(x)- \mathop{\mathbb{E}[Y|x]})(\mathop{\mathbb{E}[Y|x]}-\mathop{\mathbb{E}[Y|x]})\\<br>&amp;=0<br>\end{aligned}<br>$$<br>Now the expected cost can be expressed as<br>$$<br>\begin{aligned}<br>\mathop{\mathbb{E}[C]} &amp;= \mathop{\mathbb{E}[(f(X)-Y)^2]}\\<br>&amp;=\mathop{\mathbb{E}[(f(X)-\mathop{\mathbb{E}[Y|x]})^2]}\  reducible\ error\\<br>&amp;+ \mathop{\mathbb{E}[(\mathop{\mathbb{E}[Y|x]}-Y)^2]}\ irreducible\ error<br>\end{aligned}<br>$$</p>
<p>The first term reflects how far the trained model $f(x)$ is from the optimal model $\mathop{\mathbb{E}[Y|x]}$ and the second term reflects the inherent variability in Y given X.</p>
<h1 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h1><p>The expected mean-squared error could be expressed as<br>$$<br>\begin{aligned}<br>\mathop{\mathbb{E}[||w(D)-w||<em>2^2]} &amp;= \sum</em>{j=1}^{d} \mathop{\mathbb{E}[(w_j(D)-w_j)^2]}\\<br>&amp;= \sum_{j=1}^{d} (\mathbb{E}[(w_j(D)]-w_j)^2+\mathop{\mathbb{V}[w_j(D)]}<br>\end{aligned}<br>$$</p>
<p>The expected mean-squared error to the true weight vector $w$ decomposes into the squared bias $\mathop{\mathbb{E}[(w_j(D)]}-w_j$ and the variance.<br>We do not directly optimize the bias-variance trade-off. We cannot actually measure the bias, so we do not directly minimize these terms. This guides how we select models. If we have low bias and high variance, the model is likely to be too complex and we need to select a simpler model. And if we have high bias and low variance, the model is likely to be too simple and we need to select a more complex model.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/Introduction_microfabrication/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/Introduction_microfabrication/" class="post-title-link" itemprop="url">Introduction of Microfabrication process</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-16 17:24:36" itemprop="dateCreated datePublished" datetime="2020-09-16T17:24:36-06:00">2020-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-11 00:54:24" itemprop="dateModified" datetime="2019-09-11T00:54:24-06:00">2019-09-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="What-is-Microfabrication"><a href="#What-is-Microfabrication" class="headerlink" title="What is Microfabrication?"></a>What is Microfabrication?</h3><ul>
<li>Microfabrication is a process to impose a reproducible structure on a surface through a series of deposition, etching and patterning steps.</li>
<li>Characteristic dimensions are usually 100nm-100$\mu$m</li>
<li>The field has developed to support the microelectronics industry, but has branched to create microfluidics, MEMS etc.</li>
</ul>
<h3 id="Standard-Process-Toolkit"><a href="#Standard-Process-Toolkit" class="headerlink" title="Standard Process Toolkit"></a>Standard Process Toolkit</h3><ul>
<li>Oxidization<br>converts exposed silicon to SiO<sub>2</sub> at high temperature.</li>
<li>Lithography<br>creates controlled pattern in a polymer layer(resist) using radiation that modifies polymer solubility in developer. Usually Light Lithography.</li>
<li>Etch<br>removes material not proteceted by resist</li>
<li>Deposition<br>adds a layer to the surface. Usually gas phase.</li>
<li>Doping<br>uses high energy ion beam or diffusion to introduce electronically active impurities in areas not protected by resist.</li>
</ul>
<h3 id="Fabrication-Overview"><a href="#Fabrication-Overview" class="headerlink" title="Fabrication Overview"></a>Fabrication Overview</h3><ol>
<li>Starts with an wafer upon which an iterative sequence of process steps are performed.</li>
<li>Each step adds, removes, dopes or patterns a layer in the structure.</li>
<li>The pattern of each layer is determined by a mask(part of a mask set) and the structure is built up of patterned layers.</li>
<li>After processing, the wafer is diced into 10-1000 individual die.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/hot-point%20probe/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/hot-point%20probe/" class="post-title-link" itemprop="url">Determine n or p type semiconductor using hot-point probe</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-16 17:24:36" itemprop="dateCreated datePublished" datetime="2020-09-16T17:24:36-06:00">2020-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-17 21:25:37" itemprop="dateModified" datetime="2019-09-17T21:25:37-06:00">2019-09-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>The experiment is performed by contacting a sample wafer with a hot probe and a cold probe. The probe could be made of soldering iron. Both probes are wired to a voltage meter and the hot probe connects to the positive terminal.</p>
<h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><p>When voltage read is positive, the sample is a n-type semiconductor.<br>When voltage read is negative, the sample is a p-type semiconductor.</p>
<h3 id="Reason"><a href="#Reason" class="headerlink" title="Reason"></a>Reason</h3><p>The heat source will cause charge carriers (electrons in an n-type, electron holes in a p-type) to move away from the lead. For a n-type semiconductor, electrons will move from the hot probe to the cold probe through the substrate and hot probe lead becomes positively charged.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-16 17:21:13" itemprop="dateCreated datePublished" datetime="2020-09-16T17:21:13-06:00">2020-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-06 02:15:12" itemprop="dateModified" datetime="2019-09-06T02:15:12-06:00">2019-09-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/30/literatureReview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/30/literatureReview/" class="post-title-link" itemprop="url">Literature Review Note(continually updated)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-30 15:10:49" itemprop="dateCreated datePublished" datetime="2020-06-30T15:10:49-06:00">2020-06-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-10 14:14:22" itemprop="dateModified" datetime="2020-08-10T14:14:22-06:00">2020-08-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>

<script type="text/x-mathjax-config">
  MathJax. Hub. Config({

    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }

  }); 
</script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>

<h1 id="Deformable-GANs-for-Pose-based-Human-Image-Generation"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation"></a>Deformable GANs for Pose-based Human Image Generation</h1><p>2018-04-06</p>
<p>Deformable skip connections<br>Nearest-neighbour loss</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>To generate a human image conditioned on two different variables: the appearance of a specific person and the pose of the same person in another image.<br><img src="method_deformable.png"></p>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p><img src="result_deformable.png"></p>
<hr>
<h1 id="Dense-Intrinsic-Appearance-Flow-for-Human-Pose-Transfer"><a href="#Dense-Intrinsic-Appearance-Flow-for-Human-Pose-Transfer" class="headerlink" title="Dense Intrinsic Appearance Flow for Human Pose Transfer"></a>Dense Intrinsic Appearance Flow for Human Pose Transfer</h1><p>2019-03-27</p>
<h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_dense_intrinsic.png"><br><img src="overview_dense_intrinsic2.png"></p>
<p>18 human keypoints extracted by a pose estimator are encoded into a 18-channel binary heatmap.</p>
<p>Input: image x<sub>1</sub> , extracted pose p<sub>1</sub>, target pose p<sub>2</sub></p>
<p>The module first predicts from (p<sub>1</sub>, p<sub>2</sub>) the intrinsic 3D appearance flow <strong><em>F</em></strong> and visibility map <strong><em>V</em></strong>, then use the tuple (x<sub>1</sub>, p<sub>2</sub>, <strong><em>F</em></strong>, <strong><em>V</em></strong>) for image generation.</p>
<p>A dual-path U-Net is proposed to separately model the image and pose information.</p>
<p>A feature warping module is proposed to handle the spatial misalignment issue.</p>
<p>A pixel warping module is proposed to warp pixels in input image x1 to the target pose.</p>
<p>PatchGAN is used to score the synthesized image patches.</p>
<h2 id="Difference-from-previous-work"><a href="#Difference-from-previous-work" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Integrate implicit reasoning about 3D geometry from 2D representations only.</p>
<h2 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><p>SSIM – Structure Similarity<br>IS – Inception Score<br>FashionIS – Fashion inception score<br>ArrtRec-k – Clothing attribute retianing rate<br><img src="result_dense_intrinsic.png"></p>
<h2 id="Result-1"><a href="#Result-1" class="headerlink" title="Result"></a>Result</h2><hr>
<h1 id="Everybody-Dance-Now"><a href="#Everybody-Dance-Now" class="headerlink" title="Everybody Dance Now"></a>Everybody Dance Now</h1><p>2019-08-27</p>
<h2 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h2><h3 id="pose-detection"><a href="#pose-detection" class="headerlink" title="pose detection"></a>pose detection</h3><p>Use Open-Pose pose detector to create pose stick figures given frames from the source video.</p>
<h3 id="global-pose-normalization"><a href="#global-pose-normalization" class="headerlink" title="global pose normalization"></a>global pose normalization</h3><p>Account for differences between the source and target body shapes and locations within the frame.</p>
<h3 id="mapping-from-normalized-pose-stick-figures-to-the-target-subject"><a href="#mapping-from-normalized-pose-stick-figures-to-the-target-subject" class="headerlink" title="mapping from normalized pose stick figures to the target subject"></a>mapping from normalized pose stick figures to the target subject</h3><p>Use adversarial training to learn the mapping from the pose stick figures to images of the target person. Pose to video translation using temporal smoothing and Face GAN</p>
<h2 id="Difference-from-previous-work-1"><a href="#Difference-from-previous-work-1" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Early methods focused on creating new content by manipulating existing video footage.</p>
<p>Classic computer graphics approaches to motion transfer attempt to perform this in 3D –&gt; retargeting problem. The solutions include the use of inverse kinematic solvers and retargeting between different 3D skeletons.</p>
<p>Several approaches rely on calibrated multi-camera setups to scan a target actor and manipulate their motions through a fitted 3D model.</p>
<p>Transfer head position and facial expressions between human subjects and render results in video.</p>
<p>Neural re-rendering to enhance rendering of human motion capture for VR/AR.</p>
<hr>
<p>This paper learn to synthesize novel motions and use 2D representation.</p>
<p>Retarget full body motion</p>
<p>2D pose stick figures as inputs</p>
<p>Adress motion transfer between subjects.</p>
<h2 id="Result-2"><a href="#Result-2" class="headerlink" title="Result"></a>Result</h2><p>see paper, also has limitations.</p>
<hr>
<h1 id="Animating-Arbitrary-Objects-via-Deep-Motion-Transfer-Monkey-Net"><a href="#Animating-Arbitrary-Objects-via-Deep-Motion-Transfer-Monkey-Net" class="headerlink" title="Animating Arbitrary Objects via Deep Motion Transfer (Monkey-Net)"></a>Animating Arbitrary Objects via Deep Motion Transfer (Monkey-Net)</h1><p>2019-08-30</p>
<p>To animate an object based on the motion of a similar object in a driving video.</p>
<h2 id="Method-3"><a href="#Method-3" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_animating_arbitrary.png"></p>
<h3 id="keypoint-detector"><a href="#keypoint-detector" class="headerlink" title="keypoint detector"></a>keypoint detector</h3><p>Unsupervised keypoint detection</p>
<h3 id="dense-motion-prediction-network"><a href="#dense-motion-prediction-network" class="headerlink" title="dense motion prediction network"></a>dense motion prediction network</h3><p>Generate dense heatmaps from sparse keypoints</p>
<h3 id="motion-transfer-network"><a href="#motion-transfer-network" class="headerlink" title="motion transfer network"></a>motion transfer network</h3><p>Synthesize the output frames with the input of motion heatmaps and appearance infomation extracted from the source image.</p>
<h2 id="Difference-from-previous-work-2"><a href="#Difference-from-previous-work-2" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><h3 id="deep-video-generation"><a href="#deep-video-generation" class="headerlink" title="deep video generation"></a>deep video generation</h3><p>Closely related, using a deep learning architecture.<br>Tackle a more challenging task: image animation requires decoupling and modeling motion and content information, as well as a recombining them.</p>
<h3 id="object-animation"><a href="#object-animation" class="headerlink" title="object animation"></a>object animation</h3><p>Recycle GAN only learns the association between two specific domains.<br>This method could animate an image depicting one object without knowing at training time which object will be used in the driving video.<br>This work design a self-supervised deep network for animating static images, which is effective for generating arbitrary objects.</p>
<h2 id="Evaluation-Metrics-1"><a href="#Evaluation-Metrics-1" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><p>L1 – L1 distance for video reconstruction task</p>
<p>AKD – average keypoint distance between motion of the generated video and the ground truth video motion(Tai-Chi and Nemo dataset)</p>
<p>MKR – missing keypoint rate. percentage of keypoints detected in the ground truth frame. (Tai-Chi dataset)</p>
<p>AED – Average Euclidean Distance between a feature representation of the ground truth and the generated video frames.</p>
<p>FID – Frechet Inception Distance to evaluate the quality of individual frames.</p>
<h2 id="Result-3"><a href="#Result-3" class="headerlink" title="Result"></a>Result</h2><hr>
<h1 id="Unsupervised-Keypoint-Learning-for-Guiding-Class-Conditional-Video-Prediction"><a href="#Unsupervised-Keypoint-Learning-for-Guiding-Class-Conditional-Video-Prediction" class="headerlink" title="Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction"></a>Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction</h1><p>2019-10-04</p>
<p>A deep video prediction model conditioned on a single image and an action class.</p>
<h2 id="Method-4"><a href="#Method-4" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_unsupervised_keypoint.png"></p>
<h3 id="Learning-the-keypoints-detector-with-the-image-translator"><a href="#Learning-the-keypoints-detector-with-the-image-translator" class="headerlink" title="Learning the keypoints detector with the image translator"></a>Learning the keypoints detector with the image translator</h3><p>Learning the image translation between two frames (v, v’) in the same video. It enforces the network to automatically find the most dynamic parts of the image, which can be used as the guidance to move the object in the reference image. The difference between (v, v’) corresponds to keypoints set (k, k’).</p>
<h3 id="Learning-the-motion-generator-with-pseudo-labeled-data"><a href="#Learning-the-motion-generator-with-pseudo-labeled-data" class="headerlink" title="Learning the motion generator with pseudo-labeled data"></a>Learning the motion generator with pseudo-labeled data</h3><p><img src="method_unsupervised_keypoint.png"><br>cVAE framework conditioned on the initial keypoints and the action class.</p>
<h2 id="Difference-from-previous-work-3"><a href="#Difference-from-previous-work-3" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Deterministic models: have difficulty in generating videos for novel scenes that they have not seen before.</p>
<p>GAN and VAE show using keypoints is effective, but require frame-by-frame keypoints labeling, which limits the applicability of the method.</p>
<p>Some works use unsupervised way to train the keypoint detector and show successful results.</p>
<hr>
<p>This paper propose a robust image translator using the analogical relationship between the image and keypoints.</p>
<p>A background masking to suppress the distraction from noisy backgrounds.</p>
<p>Network trained by optimizing the variational lower bound that is comprised of the KL-divergence, the reconstruction loss and adversarial loss. </p>
<h2 id="Result-4"><a href="#Result-4" class="headerlink" title="Result"></a>Result</h2><p><img src="result_unsupervised_keypoint.png"><br>Action recognition accuracy: 68.89</p>
<hr>
<h1 id="Deep-Image-Spatial-Transformation-for-Person-Image-Generation"><a href="#Deep-Image-Spatial-Transformation-for-Person-Image-Generation" class="headerlink" title="Deep Image Spatial Transformation for Person Image Generation"></a>Deep Image Spatial Transformation for Person Image Generation</h1><p>2020-03-02</p>
<p>A differentiable global-flow local-attention framework to reassemble the inputs at the feature.</p>
<h2 id="Method-5"><a href="#Method-5" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_deep_image_spatial.png"><br><img src="overview_deep_image_spatial2.png"></p>
<p>This work combines flow-based operation with attention mechanisms.</p>
<p>The framework forces each output location to be only related to a local feature patch of sources.</p>
<p>The Flow Field Estimator is responsible for extracting the global correlations and generating flow fields.</p>
<p>The Target Image Generator is used to synthesize the final results using local attention.</p>
<p>A content-aware sampling method is proposed to calculate the local attention coefficients.</p>
<h2 id="Difference-from-previous-work-4"><a href="#Difference-from-previous-work-4" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><h3 id="Pose-guided-person-image-generation"><a href="#Pose-guided-person-image-generation" class="headerlink" title="Pose-guided person image generation"></a>Pose-guided person image generation</h3><p>Does not need any supplementary information and obtains the flow fields in an unsupervised way.</p>
<h3 id="Image-Spatial-Transformation"><a href="#Image-Spatial-Transformation" class="headerlink" title="Image Spatial Transformation"></a>Image Spatial Transformation</h3><p>Appearance flow warps image pixels instead of features and limits the model to be unable to generate new contents and capture large motions.</p>
<p>This model warps features instead of pixels and use the sampling correctness loss to constraint the flow field <strong>w</strong> to sample semantically similar regions.</p>
<h2 id="Result-5"><a href="#Result-5" class="headerlink" title="Result"></a>Result</h2><p><img src="result_deep_image_spatial.png"></p>
<hr>
<h1 id="Human-motion-transfer-from-poses-in-the-wild"><a href="#Human-motion-transfer-from-poses-in-the-wild" class="headerlink" title="Human motion transfer from poses in the wild"></a>Human motion transfer from poses in the wild</h1><p>2020-04-07</p>
<h2 id="Method-6"><a href="#Method-6" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_human_motion.png"></p>
<h3 id="Pose2Video-Network"><a href="#Pose2Video-Network" class="headerlink" title="Pose2Video Network"></a>Pose2Video Network</h3><p>pix2pixHD network. multi-frame input.</p>
<p>A pose augmentation method is proposed to minimize the traning-test gap by randomly dropping some input channels, perturb the location of joints keypoints, and elongate or shorten body part lengths in some channels.</p>
<h3 id="Texture-Refinement-Network"><a href="#Texture-Refinement-Network" class="headerlink" title="Texture Refinement Network"></a>Texture Refinement Network</h3><p>Two-stage texture refinement network architecture to achieve superior texture quality.</p>
<p>pose2image, image2image</p>
<p>Follows setting of condition GAN.</p>
<h3 id="Unified-paired-and-unpaired-learning"><a href="#Unified-paired-and-unpaired-learning" class="headerlink" title="Unified paired and unpaired learning"></a>Unified paired and unpaired learning</h3><p>A unified paired and unpaired learning strategy to improve the robustness to detection errors</p>
<h2 id="Difference-from-previous-work-5"><a href="#Difference-from-previous-work-5" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Foreground only to save network capacity and computation time and costs. Systhesized foreground be easily reused on a new<br>background. </p>
<h2 id="Result-6"><a href="#Result-6" class="headerlink" title="Result"></a>Result</h2><p><img src="result_human_motion.png"></p>
<hr>
<h1 id="MISC-Multi-condition-Injection-and-Spatially-adaptive-Compositing-for-Conditional-Person-Image-Synthesis"><a href="#MISC-Multi-condition-Injection-and-Spatially-adaptive-Compositing-for-Conditional-Person-Image-Synthesis" class="headerlink" title="MISC: Multi-condition Injection and Spatially-adaptive Compositing for Conditional Person Image Synthesis"></a>MISC: Multi-condition Injection and Spatially-adaptive Compositing for Conditional Person Image Synthesis</h1><p>2020-05-01</p>
<p>Conditional image generation and image compositing.</p>
<h2 id="Method-7"><a href="#Method-7" class="headerlink" title="Method"></a>Method</h2><p><img src="method_MISC.png"></p>
<h3 id="Conditional-person-generation-model"><a href="#Conditional-person-generation-model" class="headerlink" title="Conditional person generation model"></a>Conditional person generation model</h3><p>Visually concrete condition of geometry, pattern injection, and color injection.</p>
<h3 id="Spatially-adaptive-image-composition-model"><a href="#Spatially-adaptive-image-composition-model" class="headerlink" title="Spatially-adaptive image composition model"></a>Spatially-adaptive image composition model</h3><p>Pixel transformation method, which uses NN to estimate the contrast and brightness transformation params.<br>[1] Bor-Chun Chen and Andrew Kae. Toward realistic image<br>compositing with adversarial learning. In CVPR, 2019. 1, 2,<br>4, 5, 6, 8</p>
<h2 id="Difference-from-previous-work-6"><a href="#Difference-from-previous-work-6" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Solve the gradient vanishing problem for Spatially-adaptive compositing by removing the spatially-invariant constraint.</p>
<h2 id="Result-7"><a href="#Result-7" class="headerlink" title="Result"></a>Result</h2><p><img src="result_MISC.png"></p>
<hr>
<h1 id="Wish-You-Were-Here-Context-Aware-Human-Generation"><a href="#Wish-You-Were-Here-Context-Aware-Human-Generation" class="headerlink" title="Wish You Were Here: Context-Aware Human Generation"></a>Wish You Were Here: Context-Aware Human Generation</h1><p>2020-05-21</p>
<p>Inserting humans into existing images in a photorealistic manner while respecting the semantic context of the scene.</p>
<h2 id="Method-8"><a href="#Method-8" class="headerlink" title="Method"></a>Method</h2><p><img src="method_wish_you.png"><br><img src="method2_wish_you.png"></p>
<p>Three subnetworks are involved.</p>
<ol>
<li>The first Essence Generation Network (EGN) generates the semantic map of the new person, given the pose of other persons in the scene.</li>
<li>The second Multi-conditioning rendering network (MCRN) renders the pixels of the novel person.</li>
<li>The third Face refinement network (FRN) refines the generated face.</li>
</ol>
<h2 id="Difference-from-previous-work-7"><a href="#Difference-from-previous-work-7" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Do not require a driving pose or a semantic map to render a novel person.<br>Far less limited in the class of objects.</p>
<h2 id="Result-8"><a href="#Result-8" class="headerlink" title="Result"></a>Result</h2><p><img src="result_wish_you.png"></p>
<hr>
<h1 id="Region-adaptive-texture-enhancement-for-detailed-person-image-synthesis"><a href="#Region-adaptive-texture-enhancement-for-detailed-person-image-synthesis" class="headerlink" title="Region-adaptive texture enhancement for detailed person image synthesis"></a>Region-adaptive texture enhancement for detailed person image synthesis</h1><p>2020-05-26</p>
<p>Person image synthesis, texture enhancement</p>
<h2 id="Method-9"><a href="#Method-9" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_region_adaptive.png"><br>RATE-Net utilizes the source image for label map estimation and texure control.</p>
<p>An effective training strategy to maximize the mutual guidance between two modules.</p>
<h3 id="pose-transfer-module"><a href="#pose-transfer-module" class="headerlink" title="pose transfer module"></a>pose transfer module</h3><p>Gets a reasonable content feature map <strong><em>F<sub>t</sub></em></strong><br>Generates a coarse image under the target pose.</p>
<h3 id="texture-enhancing-module"><a href="#texture-enhancing-module" class="headerlink" title="texture enhancing module"></a>texture enhancing module</h3><p>Synthesizes a region-aware residual texture map <strong><em>R<sub>t</sub></em></strong> under the guidance of <strong><em>F<sub>t</sub></em></strong> .</p>
<p>Adaptive Instance Normalization is used to inject the textural code into the content feature map.</p>
<h3 id="Discriminators"><a href="#Discriminators" class="headerlink" title="Discriminators"></a>Discriminators</h3><p>shape discriminator <strong><em>D<sub>S</sub></em></strong> evaluates pose pairs shape consistency</p>
<p>appearance discriminator <strong><em>D<sub>A</sub></em></strong> evaluates synthesized and the source image appearance consistency.</p>
<h3 id="Training-strategy"><a href="#Training-strategy" class="headerlink" title="Training strategy"></a>Training strategy</h3><ol>
<li>Update the pose transfer module with L1 loss over coarse image.</li>
<li>Update two modules together with another texture-aware loss L2 over final output. (end to end)</li>
<li>Update discriminators for K steps. </li>
</ol>
<p>(K=3 leads to a nice balance between training speed and discriminative capability)</p>
<h2 id="Difference-from-previous-work-8"><a href="#Difference-from-previous-work-8" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>For warping-based strategy, most fine-grained features would be lost due to down-sampling.</p>
<p>U-net architecture with skip-connections, not suitable due to the structural deformation of the human body under different poses.</p>
<p>Deformable skip-connection.</p>
<p>Pixelwise mapping is time-consuming and requires additional dense annotations.</p>
<hr>
<p>This method synthesize new texture that match the style of the input image with pose transfer module and texture enhancing module.</p>
<h2 id="Result-9"><a href="#Result-9" class="headerlink" title="Result"></a>Result</h2><p><img src="result_region_adaptive.png"></p>
<hr>
<h1 id="G3AN-Disentangling-Appearance-and-Motion-for-Video-Generation"><a href="#G3AN-Disentangling-Appearance-and-Motion-for-Video-Generation" class="headerlink" title="G3AN: Disentangling Appearance and Motion for Video Generation"></a>G3AN: Disentangling Appearance and Motion for Video Generation</h1><p>2020-06-13</p>
<p>Capture the distribution of high dimensional video data and model appearance and motion in disentangled manner.</p>
<h2 id="Method-10"><a href="#Method-10" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_g3an.png"><br><img src="g3_module.png"></p>
<p>Hierarchical Generator with G3 modules<br>Factorized transposed spatio-temporal convolution. (1+2)D convolution.</p>
<p><img src="g3an_spatio_temporal_fusion.png"><br>Spatio-temporal fusion</p>
<p><img src="g3an_fsa.png"><br>Factorized spatio-temporal self-attention</p>
<p>Two-stream discriminator architecture.one takes a full video as input and one takes randomly sampled frames as input.</p>
<h2 id="Difference-from-previous-work-9"><a href="#Difference-from-previous-work-9" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><ol>
<li>MoCoGAN sampled a sequence of noise vectors as motion and a fixed noise as content, which increases the learning difficulty.</li>
<li>MoCoGAN incorporated a simple image Generator aiming at generating each frame sequentially, after which content and motion features were jointly generated. This leads to incomplete disentanglement of motion and content.</li>
</ol>
<p>This method is able to entirely decompose appearance and motion in both, latent and feature spaces.</p>
<h2 id="Result-10"><a href="#Result-10" class="headerlink" title="Result"></a>Result</h2><p><img src="result_g3an.png"></p>
<hr>
<h1 id="Controllable-Person-Image-Synthesis-with-Attribute-Decomposed-GAN"><a href="#Controllable-Person-Image-Synthesis-with-Attribute-Decomposed-GAN" class="headerlink" title="Controllable Person Image Synthesis with Attribute-Decomposed GAN"></a>Controllable Person Image Synthesis with Attribute-Decomposed GAN</h1><p>2020-06-17</p>
<h2 id="Method-11"><a href="#Method-11" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_controllable_person.png"><br><img src="overview_controllable_person2.png"><br>The core idea is to embed human attributes into the latent space as independent codes and thus achieve flexible and continuous control of attributes via mixing and interpolation operations in explicit style representations.</p>
<p>Generator embeds the target pose and source person into two latent codes via two independent pathways, called pose encoding and decomposed component encoding. Two pathways are connected by a series of style blocks, which inject the texture style of source person into the pose feature. A decoder is used to reconstructe the desired person image.</p>
<h3 id="pose-encoding"><a href="#pose-encoding" class="headerlink" title="pose encoding"></a>pose encoding</h3><p>Pose encoder consists 2 down-sampling conv layers, following the regular configuration of encoder.</p>
<h3 id="decomposed-component-encoding"><a href="#decomposed-component-encoding" class="headerlink" title="decomposed component encoding"></a>decomposed component encoding</h3><p>This module first extracts the semantic map <strong><em>S</em></strong> of source person with an human parser and converts <strong><em>S</em></strong> into a K-channel heat map. There is a binary mask <strong><em>M<sub>i</sub></em></strong> for each channel <strong><em>i</em></strong> for corresponding component.<br>A texture encoder is used to acquire the corresponding style code.</p>
<p>A fusion module(FM) is proposed as an important auxiliary module for DCE. It consists of 3 FC layers, eeffectively disentangle features and avoid conflicts between forward operation and backward feedback.</p>
<h3 id="person-image-reconstruction"><a href="#person-image-reconstruction" class="headerlink" title="person image reconstruction"></a>person image reconstruction</h3><p>N deconvolutional layers decoder, following regular decoder configuration.</p>
<h3 id="Discriminators-1"><a href="#Discriminators-1" class="headerlink" title="Discriminators"></a>Discriminators</h3><p>Two discriminators are used.<br><strong><em>D<sub>p</sub></em></strong> used to guarantee the alignment of the pose of generated image with the target pose.<br><strong><em>D<sub>t</sub></em></strong> used to ensure the similarity of the appearance texture of generated image with the source person.<br>Both implemented as PatchGAN.</p>
<h2 id="Difference-from-previous-work-10"><a href="#Difference-from-previous-work-10" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>This work is the first to achieve attribute editing in the task of person image synthesis. Not only pose guided, but also component attributes controlled.</p>
<p>The proposed method does not need any annotation of component attributes and enables automatic and unsupervised attribute separation via delicately-designed modules.</p>
<h2 id="Evaluation-Metrics-2"><a href="#Evaluation-Metrics-2" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><p>IS – Inception score.<br>SSIM – Structural Similarity.<br>CX – contextual score. Cosine distance between deep features to measure the similarity of two non-aligned images, ignoring the spatial position of the features.</p>
<h2 id="Result-11"><a href="#Result-11" class="headerlink" title="Result"></a>Result</h2><p><img src="result_controllable_person.png"></p>
<h1 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h1><h2 id="Adversarial-loss"><a href="#Adversarial-loss" class="headerlink" title="Adversarial loss"></a>Adversarial loss</h2><h2 id="Reconstruction-loss"><a href="#Reconstruction-loss" class="headerlink" title="Reconstruction loss"></a>Reconstruction loss</h2><h2 id="Perceptual-loss"><a href="#Perceptual-loss" class="headerlink" title="Perceptual loss"></a>Perceptual loss</h2><h2 id="Contextual-loss"><a href="#Contextual-loss" class="headerlink" title="Contextual loss"></a>Contextual loss</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":300,"height":350},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
