<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax. Hub. Config({      extensions: [&quot;tex2jax.js&quot;],     jax: [&quot;input&#x2F;TeX&quot;, &quot;output&#x2F;HTML-CSS&quot;],     tex2jax: {       inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ],       displayMath: [ [&#39;$$&#39;,&#39;$$&#39;]">
<meta property="og:type" content="article">
<meta property="og:title" content="Literature Review Note(continually updated)">
<meta property="og:url" content="http://example.com/2020/06/30/literatureReview/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="MathJax. Hub. Config({      extensions: [&quot;tex2jax.js&quot;],     jax: [&quot;input&#x2F;TeX&quot;, &quot;output&#x2F;HTML-CSS&quot;],     tex2jax: {       inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ],       displayMath: [ [&#39;$$&#39;,&#39;$$&#39;]">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/method_deformable.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_deformable.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_dense_intrinsic.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_dense_intrinsic2.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_dense_intrinsic.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_animating_arbitrary.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_unsupervised_keypoint.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/method_unsupervised_keypoint.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_unsupervised_keypoint.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_deep_image_spatial.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_deep_image_spatial2.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_deep_image_spatial.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_human_motion.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_human_motion.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/method_MISC.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_MISC.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/method_wish_you.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/method2_wish_you.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_wish_you.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_region_adaptive.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_region_adaptive.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_g3an.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/g3_module.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/g3an_spatio_temporal_fusion.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/g3an_fsa.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_g3an.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_controllable_person.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/overview_controllable_person2.png">
<meta property="og:image" content="http://example.com/2020/06/30/literatureReview/result_controllable_person.png">
<meta property="article:published_time" content="2020-06-30T21:10:49.000Z">
<meta property="article:modified_time" content="2020-08-10T20:14:22.863Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2020/06/30/literatureReview/method_deformable.png">

<link rel="canonical" href="http://example.com/2020/06/30/literatureReview/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Literature Review Note(continually updated) | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/30/literatureReview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Literature Review Note(continually updated)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-30 15:10:49" itemprop="dateCreated datePublished" datetime="2020-06-30T15:10:49-06:00">2020-06-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-10 14:14:22" itemprop="dateModified" datetime="2020-08-10T14:14:22-06:00">2020-08-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>

<script type="text/x-mathjax-config">
  MathJax. Hub. Config({

    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }

  }); 
</script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>

<h1 id="Deformable-GANs-for-Pose-based-Human-Image-Generation"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation"></a>Deformable GANs for Pose-based Human Image Generation</h1><p>2018-04-06</p>
<p>Deformable skip connections<br>Nearest-neighbour loss</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>To generate a human image conditioned on two different variables: the appearance of a specific person and the pose of the same person in another image.<br><img src="method_deformable.png"></p>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p><img src="result_deformable.png"></p>
<hr>
<h1 id="Dense-Intrinsic-Appearance-Flow-for-Human-Pose-Transfer"><a href="#Dense-Intrinsic-Appearance-Flow-for-Human-Pose-Transfer" class="headerlink" title="Dense Intrinsic Appearance Flow for Human Pose Transfer"></a>Dense Intrinsic Appearance Flow for Human Pose Transfer</h1><p>2019-03-27</p>
<h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_dense_intrinsic.png"><br><img src="overview_dense_intrinsic2.png"></p>
<p>18 human keypoints extracted by a pose estimator are encoded into a 18-channel binary heatmap.</p>
<p>Input: image x<sub>1</sub> , extracted pose p<sub>1</sub>, target pose p<sub>2</sub></p>
<p>The module first predicts from (p<sub>1</sub>, p<sub>2</sub>) the intrinsic 3D appearance flow <strong><em>F</em></strong> and visibility map <strong><em>V</em></strong>, then use the tuple (x<sub>1</sub>, p<sub>2</sub>, <strong><em>F</em></strong>, <strong><em>V</em></strong>) for image generation.</p>
<p>A dual-path U-Net is proposed to separately model the image and pose information.</p>
<p>A feature warping module is proposed to handle the spatial misalignment issue.</p>
<p>A pixel warping module is proposed to warp pixels in input image x1 to the target pose.</p>
<p>PatchGAN is used to score the synthesized image patches.</p>
<h2 id="Difference-from-previous-work"><a href="#Difference-from-previous-work" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Integrate implicit reasoning about 3D geometry from 2D representations only.</p>
<h2 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><p>SSIM – Structure Similarity<br>IS – Inception Score<br>FashionIS – Fashion inception score<br>ArrtRec-k – Clothing attribute retianing rate<br><img src="result_dense_intrinsic.png"></p>
<h2 id="Result-1"><a href="#Result-1" class="headerlink" title="Result"></a>Result</h2><hr>
<h1 id="Everybody-Dance-Now"><a href="#Everybody-Dance-Now" class="headerlink" title="Everybody Dance Now"></a>Everybody Dance Now</h1><p>2019-08-27</p>
<h2 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h2><h3 id="pose-detection"><a href="#pose-detection" class="headerlink" title="pose detection"></a>pose detection</h3><p>Use Open-Pose pose detector to create pose stick figures given frames from the source video.</p>
<h3 id="global-pose-normalization"><a href="#global-pose-normalization" class="headerlink" title="global pose normalization"></a>global pose normalization</h3><p>Account for differences between the source and target body shapes and locations within the frame.</p>
<h3 id="mapping-from-normalized-pose-stick-figures-to-the-target-subject"><a href="#mapping-from-normalized-pose-stick-figures-to-the-target-subject" class="headerlink" title="mapping from normalized pose stick figures to the target subject"></a>mapping from normalized pose stick figures to the target subject</h3><p>Use adversarial training to learn the mapping from the pose stick figures to images of the target person. Pose to video translation using temporal smoothing and Face GAN</p>
<h2 id="Difference-from-previous-work-1"><a href="#Difference-from-previous-work-1" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Early methods focused on creating new content by manipulating existing video footage.</p>
<p>Classic computer graphics approaches to motion transfer attempt to perform this in 3D –&gt; retargeting problem. The solutions include the use of inverse kinematic solvers and retargeting between different 3D skeletons.</p>
<p>Several approaches rely on calibrated multi-camera setups to scan a target actor and manipulate their motions through a fitted 3D model.</p>
<p>Transfer head position and facial expressions between human subjects and render results in video.</p>
<p>Neural re-rendering to enhance rendering of human motion capture for VR/AR.</p>
<hr>
<p>This paper learn to synthesize novel motions and use 2D representation.</p>
<p>Retarget full body motion</p>
<p>2D pose stick figures as inputs</p>
<p>Adress motion transfer between subjects.</p>
<h2 id="Result-2"><a href="#Result-2" class="headerlink" title="Result"></a>Result</h2><p>see paper, also has limitations.</p>
<hr>
<h1 id="Animating-Arbitrary-Objects-via-Deep-Motion-Transfer-Monkey-Net"><a href="#Animating-Arbitrary-Objects-via-Deep-Motion-Transfer-Monkey-Net" class="headerlink" title="Animating Arbitrary Objects via Deep Motion Transfer (Monkey-Net)"></a>Animating Arbitrary Objects via Deep Motion Transfer (Monkey-Net)</h1><p>2019-08-30</p>
<p>To animate an object based on the motion of a similar object in a driving video.</p>
<h2 id="Method-3"><a href="#Method-3" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_animating_arbitrary.png"></p>
<h3 id="keypoint-detector"><a href="#keypoint-detector" class="headerlink" title="keypoint detector"></a>keypoint detector</h3><p>Unsupervised keypoint detection</p>
<h3 id="dense-motion-prediction-network"><a href="#dense-motion-prediction-network" class="headerlink" title="dense motion prediction network"></a>dense motion prediction network</h3><p>Generate dense heatmaps from sparse keypoints</p>
<h3 id="motion-transfer-network"><a href="#motion-transfer-network" class="headerlink" title="motion transfer network"></a>motion transfer network</h3><p>Synthesize the output frames with the input of motion heatmaps and appearance infomation extracted from the source image.</p>
<h2 id="Difference-from-previous-work-2"><a href="#Difference-from-previous-work-2" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><h3 id="deep-video-generation"><a href="#deep-video-generation" class="headerlink" title="deep video generation"></a>deep video generation</h3><p>Closely related, using a deep learning architecture.<br>Tackle a more challenging task: image animation requires decoupling and modeling motion and content information, as well as a recombining them.</p>
<h3 id="object-animation"><a href="#object-animation" class="headerlink" title="object animation"></a>object animation</h3><p>Recycle GAN only learns the association between two specific domains.<br>This method could animate an image depicting one object without knowing at training time which object will be used in the driving video.<br>This work design a self-supervised deep network for animating static images, which is effective for generating arbitrary objects.</p>
<h2 id="Evaluation-Metrics-1"><a href="#Evaluation-Metrics-1" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><p>L1 – L1 distance for video reconstruction task</p>
<p>AKD – average keypoint distance between motion of the generated video and the ground truth video motion(Tai-Chi and Nemo dataset)</p>
<p>MKR – missing keypoint rate. percentage of keypoints detected in the ground truth frame. (Tai-Chi dataset)</p>
<p>AED – Average Euclidean Distance between a feature representation of the ground truth and the generated video frames.</p>
<p>FID – Frechet Inception Distance to evaluate the quality of individual frames.</p>
<h2 id="Result-3"><a href="#Result-3" class="headerlink" title="Result"></a>Result</h2><hr>
<h1 id="Unsupervised-Keypoint-Learning-for-Guiding-Class-Conditional-Video-Prediction"><a href="#Unsupervised-Keypoint-Learning-for-Guiding-Class-Conditional-Video-Prediction" class="headerlink" title="Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction"></a>Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction</h1><p>2019-10-04</p>
<p>A deep video prediction model conditioned on a single image and an action class.</p>
<h2 id="Method-4"><a href="#Method-4" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_unsupervised_keypoint.png"></p>
<h3 id="Learning-the-keypoints-detector-with-the-image-translator"><a href="#Learning-the-keypoints-detector-with-the-image-translator" class="headerlink" title="Learning the keypoints detector with the image translator"></a>Learning the keypoints detector with the image translator</h3><p>Learning the image translation between two frames (v, v’) in the same video. It enforces the network to automatically find the most dynamic parts of the image, which can be used as the guidance to move the object in the reference image. The difference between (v, v’) corresponds to keypoints set (k, k’).</p>
<h3 id="Learning-the-motion-generator-with-pseudo-labeled-data"><a href="#Learning-the-motion-generator-with-pseudo-labeled-data" class="headerlink" title="Learning the motion generator with pseudo-labeled data"></a>Learning the motion generator with pseudo-labeled data</h3><p><img src="method_unsupervised_keypoint.png"><br>cVAE framework conditioned on the initial keypoints and the action class.</p>
<h2 id="Difference-from-previous-work-3"><a href="#Difference-from-previous-work-3" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Deterministic models: have difficulty in generating videos for novel scenes that they have not seen before.</p>
<p>GAN and VAE show using keypoints is effective, but require frame-by-frame keypoints labeling, which limits the applicability of the method.</p>
<p>Some works use unsupervised way to train the keypoint detector and show successful results.</p>
<hr>
<p>This paper propose a robust image translator using the analogical relationship between the image and keypoints.</p>
<p>A background masking to suppress the distraction from noisy backgrounds.</p>
<p>Network trained by optimizing the variational lower bound that is comprised of the KL-divergence, the reconstruction loss and adversarial loss. </p>
<h2 id="Result-4"><a href="#Result-4" class="headerlink" title="Result"></a>Result</h2><p><img src="result_unsupervised_keypoint.png"><br>Action recognition accuracy: 68.89</p>
<hr>
<h1 id="Deep-Image-Spatial-Transformation-for-Person-Image-Generation"><a href="#Deep-Image-Spatial-Transformation-for-Person-Image-Generation" class="headerlink" title="Deep Image Spatial Transformation for Person Image Generation"></a>Deep Image Spatial Transformation for Person Image Generation</h1><p>2020-03-02</p>
<p>A differentiable global-flow local-attention framework to reassemble the inputs at the feature.</p>
<h2 id="Method-5"><a href="#Method-5" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_deep_image_spatial.png"><br><img src="overview_deep_image_spatial2.png"></p>
<p>This work combines flow-based operation with attention mechanisms.</p>
<p>The framework forces each output location to be only related to a local feature patch of sources.</p>
<p>The Flow Field Estimator is responsible for extracting the global correlations and generating flow fields.</p>
<p>The Target Image Generator is used to synthesize the final results using local attention.</p>
<p>A content-aware sampling method is proposed to calculate the local attention coefficients.</p>
<h2 id="Difference-from-previous-work-4"><a href="#Difference-from-previous-work-4" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><h3 id="Pose-guided-person-image-generation"><a href="#Pose-guided-person-image-generation" class="headerlink" title="Pose-guided person image generation"></a>Pose-guided person image generation</h3><p>Does not need any supplementary information and obtains the flow fields in an unsupervised way.</p>
<h3 id="Image-Spatial-Transformation"><a href="#Image-Spatial-Transformation" class="headerlink" title="Image Spatial Transformation"></a>Image Spatial Transformation</h3><p>Appearance flow warps image pixels instead of features and limits the model to be unable to generate new contents and capture large motions.</p>
<p>This model warps features instead of pixels and use the sampling correctness loss to constraint the flow field <strong>w</strong> to sample semantically similar regions.</p>
<h2 id="Result-5"><a href="#Result-5" class="headerlink" title="Result"></a>Result</h2><p><img src="result_deep_image_spatial.png"></p>
<hr>
<h1 id="Human-motion-transfer-from-poses-in-the-wild"><a href="#Human-motion-transfer-from-poses-in-the-wild" class="headerlink" title="Human motion transfer from poses in the wild"></a>Human motion transfer from poses in the wild</h1><p>2020-04-07</p>
<h2 id="Method-6"><a href="#Method-6" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_human_motion.png"></p>
<h3 id="Pose2Video-Network"><a href="#Pose2Video-Network" class="headerlink" title="Pose2Video Network"></a>Pose2Video Network</h3><p>pix2pixHD network. multi-frame input.</p>
<p>A pose augmentation method is proposed to minimize the traning-test gap by randomly dropping some input channels, perturb the location of joints keypoints, and elongate or shorten body part lengths in some channels.</p>
<h3 id="Texture-Refinement-Network"><a href="#Texture-Refinement-Network" class="headerlink" title="Texture Refinement Network"></a>Texture Refinement Network</h3><p>Two-stage texture refinement network architecture to achieve superior texture quality.</p>
<p>pose2image, image2image</p>
<p>Follows setting of condition GAN.</p>
<h3 id="Unified-paired-and-unpaired-learning"><a href="#Unified-paired-and-unpaired-learning" class="headerlink" title="Unified paired and unpaired learning"></a>Unified paired and unpaired learning</h3><p>A unified paired and unpaired learning strategy to improve the robustness to detection errors</p>
<h2 id="Difference-from-previous-work-5"><a href="#Difference-from-previous-work-5" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Foreground only to save network capacity and computation time and costs. Systhesized foreground be easily reused on a new<br>background. </p>
<h2 id="Result-6"><a href="#Result-6" class="headerlink" title="Result"></a>Result</h2><p><img src="result_human_motion.png"></p>
<hr>
<h1 id="MISC-Multi-condition-Injection-and-Spatially-adaptive-Compositing-for-Conditional-Person-Image-Synthesis"><a href="#MISC-Multi-condition-Injection-and-Spatially-adaptive-Compositing-for-Conditional-Person-Image-Synthesis" class="headerlink" title="MISC: Multi-condition Injection and Spatially-adaptive Compositing for Conditional Person Image Synthesis"></a>MISC: Multi-condition Injection and Spatially-adaptive Compositing for Conditional Person Image Synthesis</h1><p>2020-05-01</p>
<p>Conditional image generation and image compositing.</p>
<h2 id="Method-7"><a href="#Method-7" class="headerlink" title="Method"></a>Method</h2><p><img src="method_MISC.png"></p>
<h3 id="Conditional-person-generation-model"><a href="#Conditional-person-generation-model" class="headerlink" title="Conditional person generation model"></a>Conditional person generation model</h3><p>Visually concrete condition of geometry, pattern injection, and color injection.</p>
<h3 id="Spatially-adaptive-image-composition-model"><a href="#Spatially-adaptive-image-composition-model" class="headerlink" title="Spatially-adaptive image composition model"></a>Spatially-adaptive image composition model</h3><p>Pixel transformation method, which uses NN to estimate the contrast and brightness transformation params.<br>[1] Bor-Chun Chen and Andrew Kae. Toward realistic image<br>compositing with adversarial learning. In CVPR, 2019. 1, 2,<br>4, 5, 6, 8</p>
<h2 id="Difference-from-previous-work-6"><a href="#Difference-from-previous-work-6" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Solve the gradient vanishing problem for Spatially-adaptive compositing by removing the spatially-invariant constraint.</p>
<h2 id="Result-7"><a href="#Result-7" class="headerlink" title="Result"></a>Result</h2><p><img src="result_MISC.png"></p>
<hr>
<h1 id="Wish-You-Were-Here-Context-Aware-Human-Generation"><a href="#Wish-You-Were-Here-Context-Aware-Human-Generation" class="headerlink" title="Wish You Were Here: Context-Aware Human Generation"></a>Wish You Were Here: Context-Aware Human Generation</h1><p>2020-05-21</p>
<p>Inserting humans into existing images in a photorealistic manner while respecting the semantic context of the scene.</p>
<h2 id="Method-8"><a href="#Method-8" class="headerlink" title="Method"></a>Method</h2><p><img src="method_wish_you.png"><br><img src="method2_wish_you.png"></p>
<p>Three subnetworks are involved.</p>
<ol>
<li>The first Essence Generation Network (EGN) generates the semantic map of the new person, given the pose of other persons in the scene.</li>
<li>The second Multi-conditioning rendering network (MCRN) renders the pixels of the novel person.</li>
<li>The third Face refinement network (FRN) refines the generated face.</li>
</ol>
<h2 id="Difference-from-previous-work-7"><a href="#Difference-from-previous-work-7" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>Do not require a driving pose or a semantic map to render a novel person.<br>Far less limited in the class of objects.</p>
<h2 id="Result-8"><a href="#Result-8" class="headerlink" title="Result"></a>Result</h2><p><img src="result_wish_you.png"></p>
<hr>
<h1 id="Region-adaptive-texture-enhancement-for-detailed-person-image-synthesis"><a href="#Region-adaptive-texture-enhancement-for-detailed-person-image-synthesis" class="headerlink" title="Region-adaptive texture enhancement for detailed person image synthesis"></a>Region-adaptive texture enhancement for detailed person image synthesis</h1><p>2020-05-26</p>
<p>Person image synthesis, texture enhancement</p>
<h2 id="Method-9"><a href="#Method-9" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_region_adaptive.png"><br>RATE-Net utilizes the source image for label map estimation and texure control.</p>
<p>An effective training strategy to maximize the mutual guidance between two modules.</p>
<h3 id="pose-transfer-module"><a href="#pose-transfer-module" class="headerlink" title="pose transfer module"></a>pose transfer module</h3><p>Gets a reasonable content feature map <strong><em>F<sub>t</sub></em></strong><br>Generates a coarse image under the target pose.</p>
<h3 id="texture-enhancing-module"><a href="#texture-enhancing-module" class="headerlink" title="texture enhancing module"></a>texture enhancing module</h3><p>Synthesizes a region-aware residual texture map <strong><em>R<sub>t</sub></em></strong> under the guidance of <strong><em>F<sub>t</sub></em></strong> .</p>
<p>Adaptive Instance Normalization is used to inject the textural code into the content feature map.</p>
<h3 id="Discriminators"><a href="#Discriminators" class="headerlink" title="Discriminators"></a>Discriminators</h3><p>shape discriminator <strong><em>D<sub>S</sub></em></strong> evaluates pose pairs shape consistency</p>
<p>appearance discriminator <strong><em>D<sub>A</sub></em></strong> evaluates synthesized and the source image appearance consistency.</p>
<h3 id="Training-strategy"><a href="#Training-strategy" class="headerlink" title="Training strategy"></a>Training strategy</h3><ol>
<li>Update the pose transfer module with L1 loss over coarse image.</li>
<li>Update two modules together with another texture-aware loss L2 over final output. (end to end)</li>
<li>Update discriminators for K steps. </li>
</ol>
<p>(K=3 leads to a nice balance between training speed and discriminative capability)</p>
<h2 id="Difference-from-previous-work-8"><a href="#Difference-from-previous-work-8" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>For warping-based strategy, most fine-grained features would be lost due to down-sampling.</p>
<p>U-net architecture with skip-connections, not suitable due to the structural deformation of the human body under different poses.</p>
<p>Deformable skip-connection.</p>
<p>Pixelwise mapping is time-consuming and requires additional dense annotations.</p>
<hr>
<p>This method synthesize new texture that match the style of the input image with pose transfer module and texture enhancing module.</p>
<h2 id="Result-9"><a href="#Result-9" class="headerlink" title="Result"></a>Result</h2><p><img src="result_region_adaptive.png"></p>
<hr>
<h1 id="G3AN-Disentangling-Appearance-and-Motion-for-Video-Generation"><a href="#G3AN-Disentangling-Appearance-and-Motion-for-Video-Generation" class="headerlink" title="G3AN: Disentangling Appearance and Motion for Video Generation"></a>G3AN: Disentangling Appearance and Motion for Video Generation</h1><p>2020-06-13</p>
<p>Capture the distribution of high dimensional video data and model appearance and motion in disentangled manner.</p>
<h2 id="Method-10"><a href="#Method-10" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_g3an.png"><br><img src="g3_module.png"></p>
<p>Hierarchical Generator with G3 modules<br>Factorized transposed spatio-temporal convolution. (1+2)D convolution.</p>
<p><img src="g3an_spatio_temporal_fusion.png"><br>Spatio-temporal fusion</p>
<p><img src="g3an_fsa.png"><br>Factorized spatio-temporal self-attention</p>
<p>Two-stream discriminator architecture.one takes a full video as input and one takes randomly sampled frames as input.</p>
<h2 id="Difference-from-previous-work-9"><a href="#Difference-from-previous-work-9" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><ol>
<li>MoCoGAN sampled a sequence of noise vectors as motion and a fixed noise as content, which increases the learning difficulty.</li>
<li>MoCoGAN incorporated a simple image Generator aiming at generating each frame sequentially, after which content and motion features were jointly generated. This leads to incomplete disentanglement of motion and content.</li>
</ol>
<p>This method is able to entirely decompose appearance and motion in both, latent and feature spaces.</p>
<h2 id="Result-10"><a href="#Result-10" class="headerlink" title="Result"></a>Result</h2><p><img src="result_g3an.png"></p>
<hr>
<h1 id="Controllable-Person-Image-Synthesis-with-Attribute-Decomposed-GAN"><a href="#Controllable-Person-Image-Synthesis-with-Attribute-Decomposed-GAN" class="headerlink" title="Controllable Person Image Synthesis with Attribute-Decomposed GAN"></a>Controllable Person Image Synthesis with Attribute-Decomposed GAN</h1><p>2020-06-17</p>
<h2 id="Method-11"><a href="#Method-11" class="headerlink" title="Method"></a>Method</h2><p><img src="overview_controllable_person.png"><br><img src="overview_controllable_person2.png"><br>The core idea is to embed human attributes into the latent space as independent codes and thus achieve flexible and continuous control of attributes via mixing and interpolation operations in explicit style representations.</p>
<p>Generator embeds the target pose and source person into two latent codes via two independent pathways, called pose encoding and decomposed component encoding. Two pathways are connected by a series of style blocks, which inject the texture style of source person into the pose feature. A decoder is used to reconstructe the desired person image.</p>
<h3 id="pose-encoding"><a href="#pose-encoding" class="headerlink" title="pose encoding"></a>pose encoding</h3><p>Pose encoder consists 2 down-sampling conv layers, following the regular configuration of encoder.</p>
<h3 id="decomposed-component-encoding"><a href="#decomposed-component-encoding" class="headerlink" title="decomposed component encoding"></a>decomposed component encoding</h3><p>This module first extracts the semantic map <strong><em>S</em></strong> of source person with an human parser and converts <strong><em>S</em></strong> into a K-channel heat map. There is a binary mask <strong><em>M<sub>i</sub></em></strong> for each channel <strong><em>i</em></strong> for corresponding component.<br>A texture encoder is used to acquire the corresponding style code.</p>
<p>A fusion module(FM) is proposed as an important auxiliary module for DCE. It consists of 3 FC layers, eeffectively disentangle features and avoid conflicts between forward operation and backward feedback.</p>
<h3 id="person-image-reconstruction"><a href="#person-image-reconstruction" class="headerlink" title="person image reconstruction"></a>person image reconstruction</h3><p>N deconvolutional layers decoder, following regular decoder configuration.</p>
<h3 id="Discriminators-1"><a href="#Discriminators-1" class="headerlink" title="Discriminators"></a>Discriminators</h3><p>Two discriminators are used.<br><strong><em>D<sub>p</sub></em></strong> used to guarantee the alignment of the pose of generated image with the target pose.<br><strong><em>D<sub>t</sub></em></strong> used to ensure the similarity of the appearance texture of generated image with the source person.<br>Both implemented as PatchGAN.</p>
<h2 id="Difference-from-previous-work-10"><a href="#Difference-from-previous-work-10" class="headerlink" title="Difference from previous work"></a>Difference from previous work</h2><p>This work is the first to achieve attribute editing in the task of person image synthesis. Not only pose guided, but also component attributes controlled.</p>
<p>The proposed method does not need any annotation of component attributes and enables automatic and unsupervised attribute separation via delicately-designed modules.</p>
<h2 id="Evaluation-Metrics-2"><a href="#Evaluation-Metrics-2" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><p>IS – Inception score.<br>SSIM – Structural Similarity.<br>CX – contextual score. Cosine distance between deep features to measure the similarity of two non-aligned images, ignoring the spatial position of the features.</p>
<h2 id="Result-11"><a href="#Result-11" class="headerlink" title="Result"></a>Result</h2><p><img src="result_controllable_person.png"></p>
<h1 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h1><h2 id="Adversarial-loss"><a href="#Adversarial-loss" class="headerlink" title="Adversarial loss"></a>Adversarial loss</h2><h2 id="Reconstruction-loss"><a href="#Reconstruction-loss" class="headerlink" title="Reconstruction loss"></a>Reconstruction loss</h2><h2 id="Perceptual-loss"><a href="#Perceptual-loss" class="headerlink" title="Perceptual loss"></a>Perceptual loss</h2><h2 id="Contextual-loss"><a href="#Contextual-loss" class="headerlink" title="Contextual loss"></a>Contextual loss</h2>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2020/09/16/hello-world/" rel="next" title="Hello World">
      Hello World <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Deformable-GANs-for-Pose-based-Human-Image-Generation"><span class="nav-number">1.</span> <span class="nav-text">Deformable GANs for Pose-based Human Image Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method"><span class="nav-number">1.1.</span> <span class="nav-text">Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result"><span class="nav-number">1.2.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dense-Intrinsic-Appearance-Flow-for-Human-Pose-Transfer"><span class="nav-number">2.</span> <span class="nav-text">Dense Intrinsic Appearance Flow for Human Pose Transfer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-1"><span class="nav-number">2.1.</span> <span class="nav-text">Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work"><span class="nav-number">2.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-Metrics"><span class="nav-number">2.3.</span> <span class="nav-text">Evaluation Metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-1"><span class="nav-number">2.4.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Everybody-Dance-Now"><span class="nav-number">3.</span> <span class="nav-text">Everybody Dance Now</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-2"><span class="nav-number">3.1.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pose-detection"><span class="nav-number">3.1.1.</span> <span class="nav-text">pose detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#global-pose-normalization"><span class="nav-number">3.1.2.</span> <span class="nav-text">global pose normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mapping-from-normalized-pose-stick-figures-to-the-target-subject"><span class="nav-number">3.1.3.</span> <span class="nav-text">mapping from normalized pose stick figures to the target subject</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-1"><span class="nav-number">3.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-2"><span class="nav-number">3.3.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Animating-Arbitrary-Objects-via-Deep-Motion-Transfer-Monkey-Net"><span class="nav-number">4.</span> <span class="nav-text">Animating Arbitrary Objects via Deep Motion Transfer (Monkey-Net)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-3"><span class="nav-number">4.1.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#keypoint-detector"><span class="nav-number">4.1.1.</span> <span class="nav-text">keypoint detector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dense-motion-prediction-network"><span class="nav-number">4.1.2.</span> <span class="nav-text">dense motion prediction network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#motion-transfer-network"><span class="nav-number">4.1.3.</span> <span class="nav-text">motion transfer network</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-2"><span class="nav-number">4.2.</span> <span class="nav-text">Difference from previous work</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#deep-video-generation"><span class="nav-number">4.2.1.</span> <span class="nav-text">deep video generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#object-animation"><span class="nav-number">4.2.2.</span> <span class="nav-text">object animation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-Metrics-1"><span class="nav-number">4.3.</span> <span class="nav-text">Evaluation Metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-3"><span class="nav-number">4.4.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Unsupervised-Keypoint-Learning-for-Guiding-Class-Conditional-Video-Prediction"><span class="nav-number">5.</span> <span class="nav-text">Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-4"><span class="nav-number">5.1.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-the-keypoints-detector-with-the-image-translator"><span class="nav-number">5.1.1.</span> <span class="nav-text">Learning the keypoints detector with the image translator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-the-motion-generator-with-pseudo-labeled-data"><span class="nav-number">5.1.2.</span> <span class="nav-text">Learning the motion generator with pseudo-labeled data</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-3"><span class="nav-number">5.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-4"><span class="nav-number">5.3.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Image-Spatial-Transformation-for-Person-Image-Generation"><span class="nav-number">6.</span> <span class="nav-text">Deep Image Spatial Transformation for Person Image Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-5"><span class="nav-number">6.1.</span> <span class="nav-text">Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-4"><span class="nav-number">6.2.</span> <span class="nav-text">Difference from previous work</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pose-guided-person-image-generation"><span class="nav-number">6.2.1.</span> <span class="nav-text">Pose-guided person image generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Spatial-Transformation"><span class="nav-number">6.2.2.</span> <span class="nav-text">Image Spatial Transformation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-5"><span class="nav-number">6.3.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Human-motion-transfer-from-poses-in-the-wild"><span class="nav-number">7.</span> <span class="nav-text">Human motion transfer from poses in the wild</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-6"><span class="nav-number">7.1.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pose2Video-Network"><span class="nav-number">7.1.1.</span> <span class="nav-text">Pose2Video Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Texture-Refinement-Network"><span class="nav-number">7.1.2.</span> <span class="nav-text">Texture Refinement Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unified-paired-and-unpaired-learning"><span class="nav-number">7.1.3.</span> <span class="nav-text">Unified paired and unpaired learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-5"><span class="nav-number">7.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-6"><span class="nav-number">7.3.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MISC-Multi-condition-Injection-and-Spatially-adaptive-Compositing-for-Conditional-Person-Image-Synthesis"><span class="nav-number">8.</span> <span class="nav-text">MISC: Multi-condition Injection and Spatially-adaptive Compositing for Conditional Person Image Synthesis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-7"><span class="nav-number">8.1.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Conditional-person-generation-model"><span class="nav-number">8.1.1.</span> <span class="nav-text">Conditional person generation model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spatially-adaptive-image-composition-model"><span class="nav-number">8.1.2.</span> <span class="nav-text">Spatially-adaptive image composition model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-6"><span class="nav-number">8.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-7"><span class="nav-number">8.3.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Wish-You-Were-Here-Context-Aware-Human-Generation"><span class="nav-number">9.</span> <span class="nav-text">Wish You Were Here: Context-Aware Human Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-8"><span class="nav-number">9.1.</span> <span class="nav-text">Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-7"><span class="nav-number">9.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-8"><span class="nav-number">9.3.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Region-adaptive-texture-enhancement-for-detailed-person-image-synthesis"><span class="nav-number">10.</span> <span class="nav-text">Region-adaptive texture enhancement for detailed person image synthesis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-9"><span class="nav-number">10.1.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pose-transfer-module"><span class="nav-number">10.1.1.</span> <span class="nav-text">pose transfer module</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#texture-enhancing-module"><span class="nav-number">10.1.2.</span> <span class="nav-text">texture enhancing module</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discriminators"><span class="nav-number">10.1.3.</span> <span class="nav-text">Discriminators</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-strategy"><span class="nav-number">10.1.4.</span> <span class="nav-text">Training strategy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-8"><span class="nav-number">10.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-9"><span class="nav-number">10.3.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#G3AN-Disentangling-Appearance-and-Motion-for-Video-Generation"><span class="nav-number">11.</span> <span class="nav-text">G3AN: Disentangling Appearance and Motion for Video Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-10"><span class="nav-number">11.1.</span> <span class="nav-text">Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-9"><span class="nav-number">11.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-10"><span class="nav-number">11.3.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Controllable-Person-Image-Synthesis-with-Attribute-Decomposed-GAN"><span class="nav-number">12.</span> <span class="nav-text">Controllable Person Image Synthesis with Attribute-Decomposed GAN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-11"><span class="nav-number">12.1.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pose-encoding"><span class="nav-number">12.1.1.</span> <span class="nav-text">pose encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decomposed-component-encoding"><span class="nav-number">12.1.2.</span> <span class="nav-text">decomposed component encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#person-image-reconstruction"><span class="nav-number">12.1.3.</span> <span class="nav-text">person image reconstruction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discriminators-1"><span class="nav-number">12.1.4.</span> <span class="nav-text">Discriminators</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-from-previous-work-10"><span class="nav-number">12.2.</span> <span class="nav-text">Difference from previous work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-Metrics-2"><span class="nav-number">12.3.</span> <span class="nav-text">Evaluation Metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result-11"><span class="nav-number">12.4.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Loss"><span class="nav-number">13.</span> <span class="nav-text">Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Adversarial-loss"><span class="nav-number">13.1.</span> <span class="nav-text">Adversarial loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reconstruction-loss"><span class="nav-number">13.2.</span> <span class="nav-text">Reconstruction loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Perceptual-loss"><span class="nav-number">13.3.</span> <span class="nav-text">Perceptual loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Contextual-loss"><span class="nav-number">13.4.</span> <span class="nav-text">Contextual loss</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
